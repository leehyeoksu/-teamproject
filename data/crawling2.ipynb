{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5c122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ ì ‘ì† ì¤‘: https://rollcall.com/factbase/trump/search/\n",
      "â¬‡ï¸ ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë©° 'ì„ê¸° ê¸°ê°„' ë°ì´í„°ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤...\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_presidency_only_partial.csv | í˜„ì¬ 300ê±´\n",
      "   ... ìŠ¤í¬ë¡¤ 10íšŒ | ìˆ˜ì§‘ëœ ë°ì´í„°: 397ê±´ | no_growth=0\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_presidency_only_partial.csv | í˜„ì¬ 600ê±´\n",
      "   ... ìŠ¤í¬ë¡¤ 20íšŒ | ìˆ˜ì§‘ëœ ë°ì´í„°: 792ê±´ | no_growth=0\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_presidency_only_partial.csv | í˜„ì¬ 900ê±´\n",
      "   ... ìŠ¤í¬ë¡¤ 30íšŒ | ìˆ˜ì§‘ëœ ë°ì´í„°: 976ê±´ | no_growth=5\n",
      "ğŸ›‘ ì„ê¸° ë°ì´í„° ì¦ê°€ ì—†ìŒ(8íšŒ ì—°ì†) â†’ ì¢…ë£Œ\n",
      "\n",
      "âœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: trump_presidency_only.csv (ì´ 976ê±´)\n",
      "   (ì°¸ê³ ) processed_urls=1320 | seen_pre_2017=False\n",
      "                date                                                url  \\\n",
      "12  January 17, 2027  https://rollcall.com/factbase/trump/transcript...   \n",
      "0   January 21, 2026  https://rollcall.com/factbase/trump/transcript...   \n",
      "5   January 21, 2026  https://rollcall.com/factbase/trump/transcript...   \n",
      "6   January 21, 2026  https://rollcall.com/factbase/trump/transcript...   \n",
      "1   January 21, 2026  https://rollcall.com/factbase/trump/transcript...   \n",
      "\n",
      "                                                title  \n",
      "12                           Interview: No Transcript  \n",
      "0   Interview: Joe Kernen of CNBC Interviews Donal...  \n",
      "5   Remarks: Donald Trump Addresses a Reception fo...  \n",
      "6   Press Gaggle: Donald Trump Speaks to Reporters...  \n",
      "1   Remarks: Donald Trump Holds a Bilat with Abdel...  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 25ë…„1ì›”20ì¼ ~ 26ë…„ 1ì›” 22ì¼\n",
    "def scrape_presidency_only(\n",
    "    url=\"https://rollcall.com/factbase/trump/search/\",\n",
    "    max_scrolls=300,\n",
    "    sleep_after_scroll=1.8,\n",
    "    no_growth_limit=8,            # results ì¦ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ(ì—°ì† níšŒ)\n",
    "    checkpoint_every=300,         # nê±´ë§ˆë‹¤ ì„ì‹œ ì €ì¥\n",
    "    out_csv=\"trump_presidency_only.csv\",\n",
    "    checkpoint_csv=\"trump_presidency_only_partial.csv\",\n",
    "):\n",
    "    # -----------------------------\n",
    "    # [1] ëŒ€í†µë ¹ ì„ê¸° íŒë³„\n",
    "    # -----------------------------\n",
    "    term1_start = datetime(2017, 1, 20)\n",
    "    term1_end   = datetime(2021, 1, 20)\n",
    "    term2_start = datetime(2025, 1, 20)\n",
    "\n",
    "    def is_president_term(date_obj: datetime) -> bool:\n",
    "        if term1_start <= date_obj <= term1_end:\n",
    "            return True\n",
    "        if date_obj >= term2_start:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # -----------------------------\n",
    "    # [2] ë¸Œë¼ìš°ì € ì„¤ì •\n",
    "    # -----------------------------\n",
    "    opt = webdriver.ChromeOptions()\n",
    "    opt.add_argument(\"--headless=new\")\n",
    "    opt.add_argument(\"--no-sandbox\")\n",
    "    opt.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opt.add_argument(\"--window-size=1920,1080\")\n",
    "    opt.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)\n",
    "\n",
    "    results = []\n",
    "    processed_urls = set()\n",
    "\n",
    "    # ì¢…ë£Œ ì œì–´ìš©\n",
    "    no_growth = 0\n",
    "    prev_results_len = 0\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸŒ ì ‘ì† ì¤‘: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # í˜ì´ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°(ë„ˆë¬´ ëŠë¦¬ë©´ 15ì´ˆê¹Œì§€)\n",
    "        WebDriverWait(driver, 15).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        print(\"â¬‡ï¸ ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ë©° 'ì„ê¸° ê¸°ê°„' ë°ì´í„°ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "        scroll_count = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        # ì„ê¸° ì´ì „(2017-01-20 ì´ì „) ë‚ ì§œë¥¼ ë°œê²¬í–ˆëŠ”ì§€ ê¸°ë¡\n",
    "        seen_pre_2017 = False\n",
    "\n",
    "        while scroll_count < max_scrolls:\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            title_divs = soup.select('div[x-text=\"item.record_title\"]')\n",
    "            link_btns  = soup.select('a[title=\"View Transcript\"]')\n",
    "\n",
    "            min_len = min(len(title_divs), len(link_btns))\n",
    "\n",
    "            # ì´ë²ˆ í˜ì´ì§€ì—ì„œ íŒŒì‹±ëœ ë‚ ì§œ ì¤‘ \"ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ\" ì¶”ì (ì¢…ë£Œ íŒë‹¨ìš©)\n",
    "            oldest_dt_in_view = None\n",
    "\n",
    "            for i in range(min_len):\n",
    "                link = link_btns[i].get(\"href\")\n",
    "                if not link:\n",
    "                    continue\n",
    "                if \"rollcall.com\" not in link:\n",
    "                    link = \"https://rollcall.com\" + link\n",
    "\n",
    "                if link in processed_urls:\n",
    "                    continue\n",
    "                processed_urls.add(link)\n",
    "\n",
    "                full_text = title_divs[i].get_text(strip=True)\n",
    "\n",
    "                # ë³´í†µ \"TITLE - January 21, 2026\" í˜•íƒœ\n",
    "                if \" - \" not in full_text:\n",
    "                    continue\n",
    "\n",
    "                date_str = full_text.split(\" - \")[-1].strip()\n",
    "                try:\n",
    "                    dt_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # oldest ì¶”ì \n",
    "                if oldest_dt_in_view is None or dt_obj < oldest_dt_in_view:\n",
    "                    oldest_dt_in_view = dt_obj\n",
    "\n",
    "                # âœ… ì„ê¸° ê¸°ê°„ì´ë©´ ì €ì¥\n",
    "                if is_president_term(dt_obj):\n",
    "                    results.append({\n",
    "                        \"date\": date_str,\n",
    "                        \"url\": link,\n",
    "                        \"title\": full_text.split(\" - \")[0].strip()\n",
    "                    })\n",
    "\n",
    "                    # âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥(ì¤‘ê°„ ì €ì¥)\n",
    "                    if checkpoint_every and len(results) % checkpoint_every == 0:\n",
    "                        pd.DataFrame(results).drop_duplicates(subset=[\"url\"]).to_csv(\n",
    "                            checkpoint_csv, index=False, encoding=\"utf-8-sig\"\n",
    "                        )\n",
    "                        print(f\"ğŸ’¾ [ì¤‘ê°„ì €ì¥] {checkpoint_csv} | í˜„ì¬ {len(results)}ê±´\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # [A] ì¢…ë£Œì¡°ê±´ 1: ì„ê¸° ì´ì „ìœ¼ë¡œ ë‚´ë ¤ì™”ìœ¼ë©´ ì¢…ë£Œ\n",
    "            # -----------------------------\n",
    "            # í™”ë©´ì— ë³´ì´ëŠ” ê²ƒ ì¤‘ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œê°€ 2017-01-20 ì´ì „ì´ë©´\n",
    "            # ë” ë‚´ë ¤ê°€ë„ ì„ê¸° ë°ì´í„°ëŠ” ì ˆëŒ€ ì•ˆ ë‚˜ì˜¤ë¯€ë¡œ ì¢…ë£Œ\n",
    "            if oldest_dt_in_view is not None and oldest_dt_in_view < term1_start:\n",
    "                print(f\"ğŸ›‘ ì„ê¸° ì‹œì‘({term1_start.date()}) ì´ì „ ë‚ ì§œ({oldest_dt_in_view.date()}) ë„ë‹¬ â†’ ì¢…ë£Œ\")\n",
    "                seen_pre_2017 = True\n",
    "                break\n",
    "\n",
    "            # -----------------------------\n",
    "            # [B] ì¢…ë£Œì¡°ê±´ 2: results ì¦ê°€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "            # -----------------------------\n",
    "            if len(results) == prev_results_len:\n",
    "                no_growth += 1\n",
    "            else:\n",
    "                no_growth = 0\n",
    "                prev_results_len = len(results)\n",
    "\n",
    "            if no_growth >= no_growth_limit:\n",
    "                print(f\"ğŸ›‘ ì„ê¸° ë°ì´í„° ì¦ê°€ ì—†ìŒ({no_growth_limit}íšŒ ì—°ì†) â†’ ì¢…ë£Œ\")\n",
    "                break\n",
    "\n",
    "            # -----------------------------\n",
    "            # [C] ìŠ¤í¬ë¡¤ ë‹¤ìš´ + ë°”ë‹¥ ì²´í¬\n",
    "            # -----------------------------\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(sleep_after_scroll)\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                print(\"ğŸ‰ í˜ì´ì§€ ë°”ë‹¥ ë„ë‹¬(ë” ì´ìƒ ë¡œë“œ ì—†ìŒ) â†’ ì¢…ë£Œ\")\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "            scroll_count += 1\n",
    "\n",
    "            if scroll_count % 10 == 0:\n",
    "                print(f\"   ... ìŠ¤í¬ë¡¤ {scroll_count}íšŒ | ìˆ˜ì§‘ëœ ë°ì´í„°: {len(results)}ê±´ | no_growth={no_growth}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—ëŸ¬: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # -----------------------------\n",
    "    # [4] ìµœì¢… ì €ì¥\n",
    "    # -----------------------------\n",
    "    if not results:\n",
    "        print(\"âš ï¸ ì¡°ê±´(ì„ê¸° ê¸°ê°„)ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(results).drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "    # ë‚ ì§œ ì •ë ¬(ìµœì‹ ìˆœ)\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], format=\"%B %d, %Y\", errors=\"coerce\")\n",
    "    df = df.sort_values(\"_dt\", ascending=False).drop(columns=[\"_dt\"])\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nâœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: {out_csv} (ì´ {len(df)}ê±´)\")\n",
    "    print(f\"   (ì°¸ê³ ) processed_urls={len(processed_urls)} | seen_pre_2017={seen_pre_2017}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "df_presidency = scrape_presidency_only()\n",
    "if df_presidency is not None:\n",
    "    print(df_presidency.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4c08e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ìŠ¤í¬ë¡¤ 10íšŒ | term1 ìˆ˜ì§‘: 0 | processed=400\n",
      "... ìŠ¤í¬ë¡¤ 20íšŒ | term1 ìˆ˜ì§‘: 0 | processed=800\n",
      "... ìŠ¤í¬ë¡¤ 30íšŒ | term1 ìˆ˜ì§‘: 0 | processed=1200\n",
      "... ìŠ¤í¬ë¡¤ 40íšŒ | term1 ìˆ˜ì§‘: 0 | processed=1600\n",
      "... ìŠ¤í¬ë¡¤ 50íšŒ | term1 ìˆ˜ì§‘: 199 | processed=2000\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 300ê±´\n",
      "... ìŠ¤í¬ë¡¤ 60íšŒ | term1 ìˆ˜ì§‘: 599 | processed=2400\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 600ê±´\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 900ê±´\n",
      "... ìŠ¤í¬ë¡¤ 70íšŒ | term1 ìˆ˜ì§‘: 998 | processed=2800\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 1200ê±´\n",
      "... ìŠ¤í¬ë¡¤ 80íšŒ | term1 ìˆ˜ì§‘: 1398 | processed=3200\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 1500ê±´\n",
      "... ìŠ¤í¬ë¡¤ 90íšŒ | term1 ìˆ˜ì§‘: 1797 | processed=3600\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 1800ê±´\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 2100ê±´\n",
      "... ìŠ¤í¬ë¡¤ 100íšŒ | term1 ìˆ˜ì§‘: 2197 | processed=4000\n",
      "ğŸ’¾ [ì¤‘ê°„ì €ì¥] trump_term1_only_partial.csv | í˜„ì¬ 2400ê±´\n",
      "ğŸ›‘ 1ê¸° ì‹œì‘(2017-01-20) ì´ì „(2016-12-07) ë„ë‹¬ â†’ ì¢…ë£Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: trump_term1_only.csv (ì´ 2507ê±´)\n",
      "               date                                                url  \\\n",
      "0  January 20, 2021  https://rollcall.com/factbase/trump/transcript...   \n",
      "1  January 20, 2021  https://rollcall.com/factbase/trump/transcript...   \n",
      "2  January 19, 2021  https://rollcall.com/factbase/trump/transcript...   \n",
      "3  January 13, 2021  https://rollcall.com/factbase/trump/transcript...   \n",
      "4  January 12, 2021  https://rollcall.com/factbase/trump/transcript...   \n",
      "\n",
      "                                               title  \n",
      "0  Press Gaggle: Donald Trump Speaks to the Press...  \n",
      "1  Remarks: Donald Trump at Joint Base Andrews Be...  \n",
      "2  Speech: Donald Trump Delivers His Farewell Add...  \n",
      "3  Donald Trump Vlog: January 6th Insurrection an...  \n",
      "4  Remarks: Donald Trump Discusses the Border Wal...  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_term1_only(\n",
    "    url=\"https://rollcall.com/factbase/trump/search/\",\n",
    "    max_scrolls=600,\n",
    "    sleep_after_scroll=1.8,\n",
    "    no_growth_limit=10,\n",
    "    checkpoint_every=300,\n",
    "    out_csv=\"trump_term1_only.csv\",\n",
    "    checkpoint_csv=\"trump_term1_only_partial.csv\",\n",
    "):\n",
    "    term1_start = datetime(2017, 1, 20)\n",
    "    term1_end   = datetime(2021, 1, 20)\n",
    "\n",
    "    def in_term1(dt_obj: datetime) -> bool:\n",
    "        return term1_start <= dt_obj <= term1_end\n",
    "\n",
    "    opt = webdriver.ChromeOptions()\n",
    "    opt.add_argument(\"--headless=new\")\n",
    "    opt.add_argument(\"--no-sandbox\")\n",
    "    opt.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opt.add_argument(\"--window-size=1920,1080\")\n",
    "    opt.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)\n",
    "\n",
    "    results = []\n",
    "    processed_urls = set()\n",
    "\n",
    "    no_growth = 0\n",
    "    prev_processed_len = 0\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        scroll_count = 0\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while scroll_count < max_scrolls:\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            title_divs = soup.select('div[x-text=\"item.record_title\"]')\n",
    "            link_btns  = soup.select('a[title=\"View Transcript\"]')\n",
    "            min_len = min(len(title_divs), len(link_btns))\n",
    "\n",
    "            oldest_dt_in_view = None\n",
    "\n",
    "            for i in range(min_len):\n",
    "                link = link_btns[i].get(\"href\")\n",
    "                if not link:\n",
    "                    continue\n",
    "                if \"rollcall.com\" not in link:\n",
    "                    link = \"https://rollcall.com\" + link\n",
    "\n",
    "                if link in processed_urls:\n",
    "                    continue\n",
    "                processed_urls.add(link)\n",
    "\n",
    "                full_text = title_divs[i].get_text(strip=True)\n",
    "                if \" - \" not in full_text:\n",
    "                    continue\n",
    "\n",
    "                date_str = full_text.split(\" - \")[-1].strip()\n",
    "                try:\n",
    "                    dt_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                if oldest_dt_in_view is None or dt_obj < oldest_dt_in_view:\n",
    "                    oldest_dt_in_view = dt_obj\n",
    "\n",
    "                # âœ… 1ê¸°ë§Œ ì €ì¥\n",
    "                if in_term1(dt_obj):\n",
    "                    results.append({\n",
    "                        \"date\": date_str,\n",
    "                        \"url\": link,\n",
    "                        \"title\": full_text.split(\" - \")[0].strip()\n",
    "                    })\n",
    "\n",
    "                    if checkpoint_every and len(results) % checkpoint_every == 0:\n",
    "                        pd.DataFrame(results).drop_duplicates(subset=[\"url\"]).to_csv(\n",
    "                            checkpoint_csv, index=False, encoding=\"utf-8-sig\"\n",
    "                        )\n",
    "                        print(f\"ğŸ’¾ [ì¤‘ê°„ì €ì¥] {checkpoint_csv} | í˜„ì¬ {len(results)}ê±´\")\n",
    "\n",
    "            # âœ… ì¢…ë£Œì¡°ê±´ A: 2017-01-20 ì´ì „ìœ¼ë¡œ ë‚´ë ¤ê°€ë©´ 1ê¸° ëë‚œ ê²ƒ\n",
    "            if oldest_dt_in_view is not None and oldest_dt_in_view < term1_start:\n",
    "                print(f\"ğŸ›‘ 1ê¸° ì‹œì‘({term1_start.date()}) ì´ì „({oldest_dt_in_view.date()}) ë„ë‹¬ â†’ ì¢…ë£Œ\")\n",
    "                break\n",
    "\n",
    "            # âœ… ì¢…ë£Œì¡°ê±´ B: resultsê°€ ì•„ë‹ˆë¼ processed_urls ì¦ê°€ë¡œ íŒë‹¨ (ì¤‘ìš”!)\n",
    "            if len(processed_urls) == prev_processed_len:\n",
    "                no_growth += 1\n",
    "            else:\n",
    "                no_growth = 0\n",
    "                prev_processed_len = len(processed_urls)\n",
    "\n",
    "            if no_growth >= no_growth_limit:\n",
    "                print(f\"ğŸ›‘ ë” ì´ìƒ ìƒˆ í•­ëª© ë¡œë“œ ì—†ìŒ({no_growth_limit}íšŒ) â†’ ì¢…ë£Œ\")\n",
    "                break\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(sleep_after_scroll)\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                print(\"ğŸ‰ í˜ì´ì§€ ë°”ë‹¥ ë„ë‹¬ â†’ ì¢…ë£Œ\")\n",
    "                break\n",
    "\n",
    "            last_height = new_height\n",
    "            scroll_count += 1\n",
    "\n",
    "            if scroll_count % 10 == 0:\n",
    "                print(f\"... ìŠ¤í¬ë¡¤ {scroll_count}íšŒ | term1 ìˆ˜ì§‘: {len(results)} | processed={len(processed_urls)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    if not results:\n",
    "        print(\"âš ï¸ 1ê¸° ê¸°ê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(results).drop_duplicates(subset=[\"url\"])\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], format=\"%B %d, %Y\", errors=\"coerce\")\n",
    "    df = df.sort_values(\"_dt\", ascending=False).drop(columns=[\"_dt\"])\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {out_csv} (ì´ {len(df)}ê±´)\")\n",
    "    return df\n",
    "\n",
    "df_term1 = scrape_term1_only()\n",
    "print(df_term1.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440a3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term1.to_csv('/home/hyuksu/teamproject2/data/íŠ¸ëŸ¼í”„1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4893d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a1=pd.read_csv('/home/hyuksu/teamproject2/data/íŠ¸ëŸ¼í”„1')\n",
    "a2=pd.read_csv('/home/hyuksu/teamproject2/data/íŠ¸ëŸ¼í”„2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afc2d29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['date', 'url', 'title'], dtype='str'),\n",
       " Index(['date', 'url', 'title'], dtype='str'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.columns,a2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ffa5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.drop(columns='Unnamed: 0',inplace=True)\n",
    "a2.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccbf2811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2507, 976)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a1),len(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2669184",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.concat([a2,a1],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91a63de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3483"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dropna(inplace=True)\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fb735e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.drop(index=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e62b47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv('/home/hyuksu/teamproject2/data/total_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83f615d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     https://rollcall.com/factbase/trump/transcript...\n",
       "2     https://rollcall.com/factbase/trump/transcript...\n",
       "3     https://rollcall.com/factbase/trump/transcript...\n",
       "4     https://rollcall.com/factbase/trump/transcript...\n",
       "5     https://rollcall.com/factbase/trump/transcript...\n",
       "6     https://rollcall.com/factbase/trump/transcript...\n",
       "7     https://rollcall.com/factbase/trump/transcript...\n",
       "8     https://rollcall.com/factbase/trump/transcript...\n",
       "9     https://rollcall.com/factbase/trump/transcript...\n",
       "10    https://rollcall.com/factbase/trump/transcript...\n",
       "Name: url, dtype: str"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['url'].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
